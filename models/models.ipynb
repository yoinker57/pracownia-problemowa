{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0612512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0           1            2           3            4           5  \\\n",
      "0  1254.821777  266.308258  1253.617310  263.670959  1254.648926  263.740295   \n",
      "1  1263.405518  266.933228  1261.811890  264.107697  1263.873291  264.101013   \n",
      "2  1267.838501  269.231659  1269.913574  266.479858  1265.309082  267.033691   \n",
      "3  1261.708252  270.450470  1261.983398  267.829529  1263.558594  268.043274   \n",
      "4  1274.811523  275.577850  1272.380371  272.843079  1274.750732  272.592285   \n",
      "\n",
      "             6           7            8           9  ...           26  \\\n",
      "0  1250.108398  265.144958  1258.710938  265.316528  ...  1247.501587   \n",
      "1  1257.015259  266.034546  1269.713257  265.880768  ...  1256.756592   \n",
      "2  1273.361572  267.323059  1262.130127  268.485596  ...  1285.250122   \n",
      "3  1264.280518  269.696411  1274.004517  268.953156  ...  1267.253418   \n",
      "4  1264.942627  274.395477  1277.333008  274.184326  ...  1269.195312   \n",
      "\n",
      "           27           28          29           30          31           32  \\\n",
      "0  350.576843  1266.965942  354.060181  1239.922241  383.194000  1264.777588   \n",
      "1  352.311951  1279.668579  354.992615  1249.964478  383.825500  1277.635498   \n",
      "2  354.174438  1261.474365  355.495972  1282.185791  384.358643  1257.797241   \n",
      "3  356.102600  1286.759766  357.810547  1261.994873  387.463928  1287.077271   \n",
      "4  361.755188  1290.239014  363.805389  1266.272339  392.828979  1292.158447   \n",
      "\n",
      "           33                            movie_name         style  \n",
      "0  387.770264  trimmed_noaudio20240320142001028.mp4  intermediate  \n",
      "1  388.283997  trimmed_noaudio20240320142001028.mp4  intermediate  \n",
      "2  385.996033  trimmed_noaudio20240320142001028.mp4  intermediate  \n",
      "3  389.612762  trimmed_noaudio20240320142001028.mp4  intermediate  \n",
      "4  397.121704  trimmed_noaudio20240320142001028.mp4  intermediate  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def load(pose_path: str, labels_path: str):\n",
    "    \"\"\"\n",
    "    load data from csv files in the given path\n",
    "    \"\"\"\n",
    "    # get labels\n",
    "    df_labels = pd.read_csv(labels_path)\n",
    "\n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = [ p for p in glob(os.path.join(pose_path, '*.csv'))]\n",
    "    movie_names = [n.removeprefix(pose_path).removeprefix('/').removesuffix('_clicked.csv') + \".mp4\" for n in csv_files]\n",
    "\n",
    "    # filter out files that are not in the labels\n",
    "    df_labels = df_labels[df_labels['movieName'].isin(movie_names)]\n",
    "\n",
    "    # get labels for the movies\n",
    "    movie_labels_dict = df_labels.set_index('movieName')['SKIER_LEVEL'].to_dict()\n",
    "    movie_labels = [movie_labels_dict.get(n, 'Unknown') for n in movie_names]\n",
    "\n",
    "    # Load and concatenate all CSV files into one DataFrame\n",
    "    df = pd.concat(\n",
    "        [pd.read_csv(f).assign(movie_name=n, style=l) for f,n,l in zip(csv_files, movie_names, movie_labels)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = load('../pose_outputs_clicked', '../data/labeledFilms.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fcfd6",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac13cb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2, 40, 34), y shape: (2,)\n",
      "Epoch 1, Loss: 0.0000\n",
      "Epoch 2, Loss: 0.0000\n",
      "Epoch 3, Loss: 0.0000\n",
      "Epoch 4, Loss: 0.0000\n",
      "Epoch 5, Loss: 0.0000\n",
      "Epoch 6, Loss: 0.0000\n",
      "Epoch 7, Loss: 0.0000\n",
      "Epoch 8, Loss: 0.0000\n",
      "Epoch 9, Loss: 0.0000\n",
      "Epoch 10, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# load the data\n",
    "df = load('../pose_outputs_clicked', '../data/labeledFilms.csv')\n",
    "df['style'] = df['style'].astype(str)  # ensure it's str (for label encoding)\n",
    "\n",
    "# Prepare the data\n",
    "SEQUENCE_LENGTH = 40  # truncate or pad to this length\n",
    "FEATURE_SIZE = len(df.columns) - 2  # exclude 'style' and 'movie_name'\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for movie_name, group in df.groupby(\"movie_name\"):\n",
    "    group = group.drop(columns=[\"movie_name\"])  # keep only keypoints + label\n",
    "    label = group['style'].iloc[0]  # assume label same for the whole clip\n",
    "    keypoints = group.drop(columns=['style']).values.astype(np.float32)\n",
    "\n",
    "    # Truncate or pad\n",
    "    if len(keypoints) >= SEQUENCE_LENGTH:\n",
    "        keypoints = keypoints[:SEQUENCE_LENGTH]\n",
    "    else:\n",
    "        pad_len = SEQUENCE_LENGTH - len(keypoints)\n",
    "        padding = np.zeros((pad_len, keypoints.shape[1]), dtype=np.float32)\n",
    "        keypoints = np.vstack((keypoints, padding))\n",
    "\n",
    "    sequences.append(keypoints)\n",
    "    labels.append(label)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "X = np.array(sequences, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.int64)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")  # (n_clips, seq_len, features)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "class PoseSequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X)\n",
    "        self.y = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(PoseSequenceDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(PoseSequenceDataset(X_val, y_val), batch_size=32)\n",
    "\n",
    "class LSTMPoseClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last time step\n",
    "        return self.fc(out)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X.shape[2]\n",
    "model = LSTMPoseClassifier(input_size=input_size, hidden_size=64, num_classes=len(le.classes_))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae625653",
   "metadata": {},
   "source": [
    "## Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0f4bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        preds = model(X_batch)\n",
    "        predicted = preds.argmax(dim=1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "print(\"Validation Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe25a49",
   "metadata": {},
   "source": [
    "# DTW + k-NN\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bbf30a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for new clip: ['intermediate']\n"
     ]
    }
   ],
   "source": [
    "from tslearn.metrics import dtw\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "# Suppose each sample is a (T, D) array: T=timesteps, D=features (e.g. 34 keypoints)\n",
    "# You need to group rows per movie into a sequence\n",
    "X_sequences = []\n",
    "y = []\n",
    "\n",
    "for movie_name, group in df.groupby(\"movie_name\"):\n",
    "    coords = group.drop(columns=[\"movie_name\", \"style\"]).values  # shape (T, D)\n",
    "    X_sequences.append(coords)\n",
    "    y.append(group[\"style\"].iloc[0])\n",
    "\n",
    "# Convert to proper time series format (num_samples, T, D)\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "X_ts = to_time_series_dataset(X_sequences)  # handles padding internally\n",
    "\n",
    "# Train a classifier\n",
    "knn = KNeighborsTimeSeriesClassifier(n_neighbors=1, metric=\"dtw\")\n",
    "knn.fit(X_ts, y)\n",
    "\n",
    "# Predict on a new clip\n",
    "new_clip_sequence = np.random.rand(SEQUENCE_LENGTH, FEATURE_SIZE).astype(np.float32)  # Example new clip\n",
    "y_pred = knn.predict([new_clip_sequence])\n",
    "print(\"Predicted label for new clip:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341c69e",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a629478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
